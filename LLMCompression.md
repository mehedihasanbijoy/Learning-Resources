<h1 align="center">Compression of Large Language Models</h1>

<h4 align="left"><ins>Papers</ins></h4>

* **Confidence-Aware Multi-Teacher Knowledge Distillation** [[Paper Link](https://arxiv.org/abs/2201.00007)]
* **AQLM: Extreme Compression of Large Language Models via Additive Quantization** [[Paper Link](https://arxiv.org/html/2401.06118v2)] [[AQLM Library](https://github.com/Vahe1994/AQLM)] [[Blog](https://towardsdatascience.com/the-aqlm-quantization-algorithm-explained-8cf33e4a783e)]
* **AWQ: Activation-aware weight quantization for llm compression and acceleration** [[Paper Link](https://arxiv.org/pdf/2306.00978.pdf)]
* **GPTQ: Accurate post-training quantization for generative pre-trained transformers** [[Paper Link](https://arxiv.org/pdf/2210.17323.pdf)] [[AutoGPTQ Library](https://github.com/AutoGPTQ/AutoGPTQ)] [[GPTQ HuggingFace Integration](https://huggingface.co/blog/gptq-integration)]
* **SliceGPT: Compress large language models by deleting rows and columns** [[Paper Link](https://arxiv.org/pdf/2401.15024.pdf)]


<h4 align="left"><ins>Blogs</ins></h4>

* [**Understanding Compression of Large Language Models (LLMs)**](https://medium.com/@sasirekharameshkumar/understanding-compression-of-large-language-models-2ee3b8a350a2)
* [**Model Compression Techniques for Large Language Models**](https://ogre51.medium.com/model-compression-techniques-for-large-language-models-05d87236cfa2)

<h4 align="left"><ins>Tutorials / Talks</ins></h4>

* **Quantizing Large Language Models** [[Part 1](https://www.youtube.com/watch?v=kw7S-3s50uk&ab_channel=JulienSimon) | [Part 2](https://www.youtube.com/watch?v=fXBBwCIA0Ds&ab_channel=JulienSimon)] <img src="https://github.com/mehedihasanbijoy/Learning-Resources/assets/58245357/29836580-38bf-410b-9a8f-63fe473aab27" alt="YouTube" width="70"/>


<h4 align="left"><ins>HuggingFace</ins></h4>

* [**Overview of natively supported quantization schemes in ðŸ¤— Transformers**](https://huggingface.co/blog/overview-quantization-transformers)


<h4 align="left"><ins>GitHub</ins></h4>
